{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 import os\
from flask import Flask, request, jsonify\
from flask_cors import CORS\
from pypdf import PdfReader\
from sentence_transformers import SentenceTransformer\
from sklearn.metrics.pairwise import cosine_similarity\
from sklearn.feature_extraction.text import TfidfVectorizer\
import numpy as np\
from openai import OpenAI\
\
app = Flask(__name__)\
CORS(app)\
\
# === Config ===\
PDF_PATH = os.getenv("PDF_PATH", "ISO_9001_2015.pdf")\
EMBED_MODEL = os.getenv("EMBED_MODEL", "all-MiniLM-L6-v2")\
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")\
MODEL = "gpt-4o-mini"\
\
if not OPENAI_API_KEY:\
    raise ValueError("Missing OPENAI_API_KEY in environment variables.")\
\
client = OpenAI(api_key=OPENAI_API_KEY)\
\
# === Load PDF ===\
def read_pdf_text(path):\
    reader = PdfReader(path)\
    text = ""\
    for page in reader.pages:\
        text += page.extract_text() or ""\
    return text\
\
def chunk_text(text, chunk_size=1200, overlap=200):\
    chunks = []\
    for i in range(0, len(text), chunk_size - overlap):\
        chunks.append(text[i:i + chunk_size].strip())\
    return chunks\
\
print("\uc0\u55357 \u56633  Loading model & PDF...")\
model = SentenceTransformer(EMBED_MODEL)\
text = read_pdf_text(PDF_PATH)\
chunks = chunk_text(text)\
print(f"Loaded \{len(chunks)\} chunks from \{PDF_PATH\}")\
\
embeddings = model.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)\
vectorizer = TfidfVectorizer(stop_words="english", ngram_range=(1, 2), min_df=2)\
tfidf = vectorizer.fit_transform(chunks)\
print("\uc0\u9989  Index ready.")\
\
# === Helper: retrieve relevant text ===\
def retrieve_context(question, top_k=5):\
    qv = model.encode([question], convert_to_numpy=True, normalize_embeddings=True)\
    dense = (embeddings @ qv.T).ravel()\
    qtfidf = vectorizer.transform([question])\
    sparse = cosine_similarity(tfidf, qtfidf).ravel()\
    scores = 0.6 * dense + 0.4 * sparse\
    idx = np.argsort(-scores)[:top_k]\
    return "\\n\\n".join(chunks[i] for i in idx)\
\
# === Endpoint ===\
@app.route("/ask", methods=["POST"])\
def ask():\
    data = request.get_json() or \{\}\
    q = data.get("question", "").strip()\
    if not q:\
        return jsonify(\{"error": "No question provided"\}), 400\
\
    context = retrieve_context(q)\
\
    system_prompt = (\
        "You are an ISO 9001:2015 compliance assistant. "\
        "Answer the user's question **only** using the document text below. "\
        "If unsure, say you cannot find that information. "\
        "Always keep answers concise and reference relevant clause numbers if present."\
    )\
\
    messages = [\
        \{"role": "system", "content": system_prompt\},\
        \{"role": "user", "content": f"Question: \{q\}\\n\\nDocument:\\n\{context\}"\}\
    ]\
\
    try:\
        response = client.chat.completions.create(\
            model=MODEL,\
            messages=messages,\
            temperature=0.2,\
            max_tokens=400\
        )\
        answer = response.choices[0].message.content.strip()\
        return jsonify(\{"answer": answer\})\
    except Exception as e:\
        return jsonify(\{"error": str(e)\}), 500\
\
@app.route("/", methods=["GET"])\
def root():\
    return jsonify(\{"ok": True, "message": "POST /ask \{question:'...'\}"\})\
\
if __name__ == "__main__":\
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 8000)))}